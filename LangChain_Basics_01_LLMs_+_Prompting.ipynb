{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XoJ-RGiUo-uJ"
      },
      "source": [
        "# Langchain: The basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RSTxnoIozRN",
        "outputId": "c8a633cc-3792-46b4-f794-8cbdc843993e"
      },
      "outputs": [],
      "source": [
        "!pip -q install openai langchain huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9iIacDfgpB2M"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = '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'\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-KB9qA8bpxgJ"
      },
      "source": [
        "## Plain Conditional Generation\n",
        "\n",
        "### First with OpenAI GPT3 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-lzO5PfUpwfv"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTiEn3tKp7mZ"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(model_name='text-davinci-003', \n",
        "             temperature=0.9, \n",
        "             max_tokens = 256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCBfxD4cqXsx",
        "outputId": "48016d04-0d0c-4d91-ffe9-def926c857fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "To get to the other side.\n"
          ]
        }
      ],
      "source": [
        "text = \"Why did the duck cross the road?\"\n",
        "\n",
        "print(llm(text))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lCx_zw5dqxH3"
      },
      "source": [
        "### Now with T5-Flan-XL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZYdStv_rSVU"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import HuggingFaceHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swswqGCyqi7A"
      },
      "outputs": [],
      "source": [
        "\n",
        "llm_hf = HuggingFaceHub(\n",
        "    repo_id=\"google/flan-t5-xl\",\n",
        "    model_kwargs={\"temperature\":0.9 }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUwUR9U7qkld",
        "outputId": "db7f444f-22e2-45bc-dd0f-76fbd8debcab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It was hungry.\n"
          ]
        }
      ],
      "source": [
        "text = \"Why did the chicken cross the road?\"\n",
        "\n",
        "print(llm_hf(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKRIRQwlrgKy"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xidOhWp7rk_5"
      },
      "source": [
        "## Prompt Templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWFJY6-Qrm0L"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "\n",
        "restaurant_template = \"\"\"\n",
        "I want you to act as a naming consultant for new restaurants.\n",
        "\n",
        "Return a list of restaurant names. Each name should be short, catchy and easy to remember. It shoud relate to the type of restaurant you are naming.\n",
        "\n",
        "What are some good names for a restaurant that is {restaurant_desription}?\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"restaurant_desription\"],\n",
        "    template=restaurant_template,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQ0EEAywYkAb"
      },
      "outputs": [],
      "source": [
        "# An example prompt with one input variable\n",
        "prompt_template = PromptTemplate(input_variables=[\"restaurant_desription\"], template=restaurant_template)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "qB3E-mPeYkH-",
        "outputId": "d73087f3-1f8e-4cdc-9319-db566cecc277"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nI want you to act as a naming consultant for new restaurants.\\n\\nReturn a list of restaurant names. Each name should be short, catchy and easy to remember. It shoud relate to the type of restaurant you are naming.\\n\\nWhat are some good names for a restaurant that is a Greek place that serves fresh lamb souvlakis and other Greek food ?\\n'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "description = \"a Greek place that serves fresh lamb souvlakis and other Greek food \"\n",
        "description_02 = \"a burger place that is themed with baseball memorabilia\"\n",
        "description_03 = \"a cafe that has live hard rock music and memorabilia\"\n",
        "\n",
        "## to see what the prompt will be like\n",
        "prompt_template.format(restaurant_desription=description)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtuuvvmTayhz",
        "outputId": "ed1889f6-2ec0-4e99-cbe3-bf0cc9fea06a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "1. Rockin' Cafe \n",
            "2. Guitar Grind \n",
            "3. Rockin' Roost \n",
            "4. Electric Brew \n",
            "5. Jammin' Java \n",
            "6. Rocker's Bistro \n",
            "7. Live & Loud Cafe \n",
            "8. Amp'd Up Cafe \n",
            "9. Rock Stop Cafe \n",
            "10. Amp'd Brews & Bites\n"
          ]
        }
      ],
      "source": [
        "## querying the model with the prompt template\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "# Run the chain only specifying the input variable.\n",
        "print(chain.run(description_03))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3aiOsgwJX_Ol"
      },
      "source": [
        "## with Few Shot Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2AncvoJxON6"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate, FewShotPromptTemplate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WOFpG-RxOVb"
      },
      "outputs": [],
      "source": [
        "# First, create the list of few shot examples.\n",
        "examples = [\n",
        "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
        "    {\"word\": \"tall\", \"antonym\": \"short\"},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkDsAyF3xS7b"
      },
      "outputs": [],
      "source": [
        "# Next, we specify the template to format the examples we have provided.\n",
        "# We use the `PromptTemplate` class for this.\n",
        "example_formatter_template = \"\"\"\n",
        "Word: {word}\n",
        "Antonym: {antonym}\\n\n",
        "\"\"\"\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"word\", \"antonym\"],\n",
        "    template=example_formatter_template,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihj7fUsDxTGb"
      },
      "outputs": [],
      "source": [
        "# Finally, we create the `FewShotPromptTemplate` object.\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    # These are the examples we want to insert into the prompt.\n",
        "    examples=examples,\n",
        "    # This is how we want to format the examples when we insert them into the prompt.\n",
        "    example_prompt=example_prompt,\n",
        "    # The prefix is some text that goes before the examples in the prompt.\n",
        "    # Usually, this consists of intructions.\n",
        "    prefix=\"Give the antonym of every input\",\n",
        "    # The suffix is some text that goes after the examples in the prompt.\n",
        "    # Usually, this is where the user input will go\n",
        "    suffix=\"Word: {input}\\nAntonym:\",\n",
        "    # The input variables are the variables that the overall prompt expects.\n",
        "    input_variables=[\"input\"],\n",
        "    # The example_separator is the string we will use to join the prefix, examples, and suffix together with.\n",
        "    example_separator=\"\\n\\n\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJuHdj9wxNFq",
        "outputId": "8fa4eb09-2510-4d20-96db-3f468cf56260"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Give the antonym of every input\n",
            "\n",
            "\n",
            "Word: happy\n",
            "Antonym: sad\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Word: tall\n",
            "Antonym: short\n",
            "\n",
            "\n",
            "\n",
            "Word: big\n",
            "Antonym:\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# We can now generate a prompt using the `format` method.\n",
        "print(few_shot_prompt.format(input=\"big\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDC56SM8FEzu",
        "outputId": "cf8892d1-6c03-4c5a-c918-21fc283ed4a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Small\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=few_shot_prompt)\n",
        "\n",
        "# Run the chain only specifying the input variable.\n",
        "print(chain.run(\"Big\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "WOLFRAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pStpc2HIFY-9"
      },
      "outputs": [],
      "source": [
        "os.environ[\"WOLFRAM_ALPHA_APPID\"] = \"KT8HGE-UWXEUPXX7K\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
            "   command: /Users/divya/opt/anaconda3/envs/pytorch/bin/python /Users/divya/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/pip/_vendor/pep517/in_process/_in_process.py build_wheel /var/folders/dg/wjk8kyqn0w521m7zphy4c08c0000gn/T/tmpd_e7o_tc\n",
            "       cwd: /private/var/folders/dg/wjk8kyqn0w521m7zphy4c08c0000gn/T/pip-install-crgdesao/hnswlib_e559157795724add99a0d61cc6c0ce25\n",
            "  Complete output (61 lines):\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_ext\n",
            "  creating var\n",
            "  creating var/folders\n",
            "  creating var/folders/dg\n",
            "  creating var/folders/dg/wjk8kyqn0w521m7zphy4c08c0000gn\n",
            "  creating var/folders/dg/wjk8kyqn0w521m7zphy4c08c0000gn/T\n",
            "  gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/divya/opt/anaconda3/envs/pytorch/include -arch x86_64 -I/Users/divya/opt/anaconda3/envs/pytorch/include -arch x86_64 -I/Users/divya/opt/anaconda3/envs/pytorch/include/python3.8 -c /var/folders/dg/wjk8kyqn0w521m7zphy4c08c0000gn/T/tmpdp8k9dc7.cpp -o var/folders/dg/wjk8kyqn0w521m7zphy4c08c0000gn/T/tmpdp8k9dc7.o -std=c++14\n",
            "  xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun\n",
            "  gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/divya/opt/anaconda3/envs/pytorch/include -arch x86_64 -I/Users/divya/opt/anaconda3/envs/pytorch/include -arch x86_64 -I/Users/divya/opt/anaconda3/envs/pytorch/include/python3.8 -c /var/folders/dg/wjk8kyqn0w521m7zphy4c08c0000gn/T/tmpgqy_ezg2.cpp -o var/folders/dg/wjk8kyqn0w521m7zphy4c08c0000gn/T/tmpgqy_ezg2.o -std=c++11\n",
            "  xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun\n",
            "  Traceback (most recent call last):\n",
            "    File \"/Users/divya/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 349, in <module>\n",
            "      main()\n",
            "    File \"/Users/divya/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 331, in main\n",
            "      json_out['return_val'] = hook(**hook_input['kwargs'])\n",
            "    File \"/Users/divya/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 248, in build_wheel\n",
            "      return _build_backend().build_wheel(wheel_directory, config_settings,\n",
            "    File \"/private/var/folders/dg/wjk8kyqn0w521m7zphy4c08c0000gn/T/pip-build-env-k_vdotfx/overlay/lib/python3.8/site-packages/setuptools/build_meta.py\", line 416, in build_wheel\n",
            "      return self._build_with_temp_dir(['bdist_wheel'], '.whl',\n",
            "    File \"/private/var/folders/dg/wjk8kyqn0w521m7zphy4c08c0000gn/T/pip-build-env-k_vdotfx/overlay/lib/python3.8/site-packages/setuptools/build_meta.py\", line 401, in _build_with_temp_dir\n",
            "      self.run_setup()\n",
            "    File \"/private/var/folders/dg/wjk8kyqn0w521m7zphy4c08c0000gn/T/pip-build-env-k_vdotfx/overlay/lib/python3.8/site-packages/setuptools/build_meta.py\", line 338, in run_setup\n",
            "      exec(code, locals())\n",
            "    File \"<string>\", line 116, in <module>\n",
            "    File \"/private/var/folders/dg/wjk8kyqn0w521m7zphy4c08c0000gn/T/pip-build-env-k_vdotfx/overlay/lib/python3.8/site-packages/setuptools/__init__.py\", line 107, in setup\n",
            "      return distutils.core.setup(**attrs)\n",
            "    File \"/private/var/folders/dg/wjk8kyqn0w521m7zphy4c08c0000gn/T/pip-build-env-k_vdotfx/overlay/lib/python3.8/site-packages/setuptools/_distutils/core.py\", line 185, in setup\n",
            "      return run_commands(dist)\n",
            "    File \"/private/var/folders/dg/wjk8kyqn0w521m7zphy4c08c0000gn/T/pip-build-env-k_vdotfx/overlay/lib/python3.8/site-packages/setuptools/_distutils/core.py\", line 201, in run_commands\n",
            "      dist.run_commands()\n",
            "    File \"/private/var/folders/dg/wjk8kyqn0w521m7zphy4c08c0000gn/T/pip-build-env-k_vdotfx/overlay/lib/python3.8/site-packages/setuptools/_distutils/dist.py\", line 969, in run_commands\n",
            "      self.run_command(cmd)\n",
            "    File \"/private/var/folders/dg/wjk8kyqn0w521m7zphy4c08c0000gn/T/pip-build-env-k_vdotfx/overlay/lib/python3.8/site-packages/setuptools/dist.py\", line 1244, in run_command\n",
            "      super().run_command(command)\n",
            "    File \"/private/var/folders/dg/wjk8kyqn0w521m7zphy4c08c0000gn/T/pip-build-env-k_vdotfx/overlay/lib/python3.8/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
            "      cmd_obj.run()\n",
            "    File \"/private/var/folders/dg/wjk8kyqn0w521m7zphy4c08c0000gn/T/pip-build-env-k_vdotfx/overlay/lib/python3.8/site-packages/wheel/bdist_wheel.py\", line 343, in run\n",
            "      self.run_command(\"build\")\n",
            "    File \"/private/var/folders/dg/wjk8kyqn0w521m7zphy4c08c0000gn/T/pip-build-env-k_vdotfx/overlay/lib/python3.8/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n",
            "      self.distribution.run_command(command)\n",
            "    File \"/private/var/folders/dg/wjk8kyqn0w521m7zphy4c08c0000gn/T/pip-build-env-k_vdotfx/overlay/lib/python3.8/site-packages/setuptools/dist.py\", line 1244, in run_command\n",
            "      super().run_command(command)\n",
            "    File \"/private/var/folders/dg/wjk8kyqn0w521m7zphy4c08c0000gn/T/pip-build-env-k_vdotfx/overlay/lib/python3.8/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
            "      cmd_obj.run()\n",
            "    File \"/private/var/folders/dg/wjk8kyqn0w521m7zphy4c08c0000gn/T/pip-build-env-k_vdotfx/overlay/lib/python3.8/site-packages/setuptools/_distutils/command/build.py\", line 131, in run\n",
            "      self.run_command(cmd_name)\n",
            "    File \"/private/var/folders/dg/wjk8kyqn0w521m7zphy4c08c0000gn/T/pip-build-env-k_vdotfx/overlay/lib/python3.8/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n",
            "      self.distribution.run_command(command)\n",
            "    File \"/private/var/folders/dg/wjk8kyqn0w521m7zphy4c08c0000gn/T/pip-build-env-k_vdotfx/overlay/lib/python3.8/site-packages/setuptools/dist.py\", line 1244, in run_command\n",
            "      super().run_command(command)\n",
            "    File \"/private/var/folders/dg/wjk8kyqn0w521m7zphy4c08c0000gn/T/pip-build-env-k_vdotfx/overlay/lib/python3.8/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
            "      cmd_obj.run()\n",
            "    File \"/private/var/folders/dg/wjk8kyqn0w521m7zphy4c08c0000gn/T/pip-build-env-k_vdotfx/overlay/lib/python3.8/site-packages/setuptools/command/build_ext.py\", line 84, in run\n",
            "      _build_ext.run(self)\n",
            "    File \"/private/var/folders/dg/wjk8kyqn0w521m7zphy4c08c0000gn/T/pip-build-env-k_vdotfx/overlay/lib/python3.8/site-packages/setuptools/_distutils/command/build_ext.py\", line 345, in run\n",
            "      self.build_extensions()\n",
            "    File \"<string>\", line 103, in build_extensions\n",
            "    File \"<string>\", line 70, in cpp_flag\n",
            "  RuntimeError: Unsupported compiler -- at least C++11 support is needed!\n",
            "  ----------------------------------------\u001b[0m\n",
            "\u001b[31m  ERROR: Failed building wheel for hnswlib\u001b[0m\n",
            "\u001b[31mERROR: Could not build wheels for hnswlib which use PEP 517 and cannot be installed directly\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip -q install wolframalpha tiktoken chromadb pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Assumption: 2 x + 5 = -3 x + 7 \\nAnswer: x = 2/5'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.utilities.wolfram_alpha import WolframAlphaAPIWrapper\n",
        "wolfram = WolframAlphaAPIWrapper()\n",
        "wolfram.run(\"What is 2x+5 = -3x + 7?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
